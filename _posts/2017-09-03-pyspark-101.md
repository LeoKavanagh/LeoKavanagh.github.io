# PySpark Tutorial

Short list of how to do basic things like read from a Hive database table to create a PySpark DataFrame, manipulate the data, and write your outputs to HDFS. 

These methods should be the same whether you have 10 rows or 10 million rows in the DataFrame.
See [Spark Programming Guide](https://spark.apache.org/docs/2.0.2/sql-programming-guide.html) for more.

 * Table of Contents
 {:toc}

## Getting Started
Open the Pyspark Console
```
PYSPARK_PYTHON="/opt/anaconda/anaconda3/bin/python" pyspark --name="PySpark Tutorial Shell"
```

Import some basic stuff
```{python}
from pyspark.sql.functions import col, countDistinct, when, trim,isnull, concat_ws, concat, lit, substring, round, udf, count
from pyspark.sql.types import StringType, DoubleType, FloatType
import math
from functools import reduce
```

Set log level just to FATAL ERRORS.
Otherwise you are spammed with INFO messages that make it impossible to see what's going on
```{python}
sc.setLogLevel("FATAL")
```

Read data from a Hive table:
```{sql}
df1 = sqlContext.sql("SELECT * FROM database_name.spark_tutorial_view")
df1.show()
```

We can also create some data locally and them make a distributed DataFrame out of it. We will make a small one for the sake of speed.
```{python}
headers = ("id", "some_text")
data=[
        (1, "all_lower_case_1"),
        (2, "asljkwq3j4flqkje342"),
        (3, "it was the best of times, it was the worst of times"),
        (4, "May the fourth be with you"),
        (5, "The fifth element"),
        (6, "all_lower_case_1"),
     ]
df = spark.createDataFrame(data, headers)

# Print the first few lines
df.show()
```

## Straightforward summaries, aggregation, filtering etc

```{python}
# Group by and aggregate
df.groupby('id').agg(count('*')).show()

# Count distinct
df.select('some_text').distinct().count()

# Filter according to some rule: show only the even-numbered rows
df.filter(df.id % 2 == 0).show()
```

## Manipulating Data in DataFrames
### UDFs
You can't just apply a function to a DataFrame column like you can
in R or pandas. Instead you need to declare it as a UDF, and give it a return type

 * Step 1: Declare a function
```{python}
def add_one(x):
    # some complicated analysis or manipulation of the data
    y = x + 1.0
    return y
```

 * Step 2: Turn this local function into a UDF, with a return type (float, double, string etc) so that it can be used on the distributed DataFrame
```{python}
udf_add_one = udf(add_one, FloatType())
```

The function you pass to udf() can be a lambda instead of a "full" function, but it still needs a return type 

```{python}
udf_upper = udf(lambda x: x.upper(), StringType())
udf_log = udf(lambda x: math.log(x), FloatType())
```

### Adding New Columns to DataFrames
Add a new column "id_plus_one" to the DataFrame df, using the UDF we defined.


We use `withColumn("some_column_name", some_value)` to add or overwrite a column on a DataFrame. If a column named `some_column_name` does not already exist in the DataFrame, we add this column to the DataFrame. Otherwise, the values in this column overwritten with `some_value`.


The end result in both cases is that the DataFrame has a column called `some_column_name`, and the value of the nth row of `some_column_name` is the nth value returned by `some_value`. You need to wrap the column name you are accessing in the UDF in col().

```{python}
new_df = df.withColumn("id_plus_one", udf_add_one(col("id")))
new_df = new_df.withColumn("id_plus_two", udf_add_one(col("id_plus_one")))
new_df.show()
```


### Manipulate a few columns in the one go.
This is more of a Python trick than an "Intro to PySpark" topic. Nevertheless, it might be handy
```{python}
names = ["id_plus_one", "id_plus_two"]
new_df = reduce(lambda new_df, idx: new_df.withColumn("log_" + names[idx], udf_log(col(names[idx]))), 
                range(len(names)), 
                new_df)

new_df.show()
```

### String Manipulation.

We are using a UDF to turn all the text in the column `some_text` to uppercase. You could do more sophisticated stuff here. The process is the same:
```{python}
new_df = new_df.withColumn("some_text", udf_upper(col("some_text")))
new_df.show()
```

## Extracting Data From DataFrames For Non-Distributed Work

Here we take the column `id_plus_one` from `new_df`, calculate its exponential, and pull the results out of the DataFrame and into a list. The list is not distributed across all the nodes on the cluster. Presumably you will then do some analysis on this locally.


There are a few things going on at once here.
 * For every row of `new_df`
 * Take the value in column `id_plus_one`
 * Apply the exponential function to it

Finally, the `collect()` method pulls the results into a list.

```{python}
exps = new_df.rdd.map(lambda x: math.exp(x.id_plus_one)).collect()
print("{}".format(exps))
```

## Make a distributed DataFrame out of local data (again)
You can't make a DF out of a list of single items: everything needs an ID.

Aside: `list(enumerate(x))` changes `[x0, x1, x2, ...]` into `[(0, x0), (1, x1), (2, x2), ...]`. This is enough to create a DF
```{python}
local_data = list(enumerate(exps))[:3]
```

In `local_data`, we have just created a list of (index, value) pairs. This can be turned into a DataFrame.


Create the DataFrame, just like at the start of the tutorial.
```{python}
former_list = spark.createDataFrame(local_data, ["id", "exps"])
former_list.show()
```

Then create a new column (just because). Out of laziness, we defined the UDF and applied it to a column all in the one go. This is quite hard to read and to debug, and hence is **VERY BAD CODE**.
```{python}
former_list = former_list.withColumn("logs", udf(lambda x: math.log(x), FloatType())(col("exps")))
```

Finally, update an existing column. We do this by using withColumn("logs") both times.
```{python}
former_list = former_list.withColumn("logs", udf(lambda x: math.log(x), FloatType())(col("logs")))
```

## Joins


### Inner Join

Simple inner join of the two DataFrames. We are joining on `new_df.id == former_list.id`.
```{python}
in_both = former_list.join(new_df, "id", "inner")
in_both.show()
```

### Left-Anti Join

Make a DataFrame containing the ids that appear in one DataFrame, but not the other. Specifically, in `new_df` only, not in `former_list`.


In SQL you would do this with `WHERE NOT EXISTS(SELECT id FROM ...)`, or `LEFT OUTER JOIN former_list WHERE former_list.id IS NULL`.
```{python}
only_in_new_df = new_df.join(former_list, "id", "leftanti")
only_in_new_df.show()
```

## Write DataFrame to HDFS.

Simple stuff. Just give the HDFS file path.


PARQUET and CSV file formats seem to work out of the box. AVRO requires a little bit more fiddling in Spark 2.0.
```{python}
only_in_new_df.rdd.saveAsTextFile("/path/to/my/hdfs/location/pyspark_tutorial/only_in_new_df.parquet")
```

