R, SparkR, sparklyr/dplyr Notes
===============================

Sparkylr and dplyr syntax are more or less identical, so if you can do
something row-wise in a normal R dplyr context, you can probably do it in
sparklyr too.

## Count occurrences of a categorical variable 
-------------------------------------------

### sparklyr/dplyr

```R 
iris %>% count(Species)
```

### SQL

```sql
SELECT Species, count(*) FROM iris GROUP BY Species
```

### Base R
```R
table(iris$Species)
```
## Count distinct
--------------

### sparklyr/dplyr

```R
iris %>%
    summarize(num_levels = n_distinct(Species))
```

### SQL

```SQL
SELECT COUNT(DISTINCT Species) AS num_levels FROM iris
```

### Base R 

```R
ct <- length(unique(iris$Species))
```

## Distinct values

### sparklyr/dplyr

```r
iris %>% distinct(Species)
```
### SQL
```sql
SELECT DISTINCT Species FROM iris
```
### Base R
```r
unique(iris$Species)
```

### SparkR
```r
distinct(iris[,'Species']) # SparkR DataFrame
```

### Change an R dataframe into a SparkR DataFrame

See https://spark.rstudio.com/,
https://spark.rstudio.com/deployment.html

```r
library(sparklyr) library(dplyr)

sc <- spark_connect(master = "local") # or 'url.to.remote.spark.cluster'

iris_spark <- copy_to(sc, iris)
```

This method silently changes full stops to underscores in column names.
Eg `iris$Sepal.Width` becomes `iris_spark$Sepal_Width` This is because
Spark (which is built in Scala) can't handle full stops in variable
names.

## Partitioning Spark DataFrames
-----------------------------

### SparkR

```r
getNumPartitions(df1) df1 <- repartition(df1, 100) getNumPartitions(df1)
```

### sparklyr

```r
df1 <- sdf_repartition(df1, partitions=100, partition_by=column_name) sdf_num_partitions(df1)
```

Note that in the sparklyr case, there is also a function called
`sdf_partition` which is used to split a DataFrame into training,
validation and testing sets. This is not what we want. We want to use
`sdf_repartition` to split the data across the different nodes in the
Spark cluster.

## Add or alter a Spark DataFrame column 
-----------------------------------------

### sparklyr

```r
# Add a new column iris\_spark %\>%
mutate(sum_everything = Sepal.Length + Sepal.Width + Petal.Length +
Petal.Width)

# Update a column

iris_spark <- iris_spark %>% mutate(Petal_Width = exp(Petal_Width)

# Update a column, changing its type

iris_spark <- iris_spark %>% mutate(Sepal_Length =
paste0('string_now_', Sepal_Length))

```

Doing this with `sparklyr`/`dplyr`/`magrittr` means that you don't have to
mess around with schemas and `dapply` like you would in SparkR.

### SparkR

Use the withColumn() function, similar to PySpark but with some extra
fluff.

```r
# Create a standard SparkR DataFrame
sdf <- createDataFrame(mtcars)

# Change one of the columns from DoubleType to StringType for the craic

sdf2 <- withColumn(sdf, 'cyl', cast(sdf$cyl, 'string'))

# Check:
printSchema(sdf)
printSchema(sdf2)

# Add a new column without collecting or passing a schema as an argument
sdf2 <- withColumn(sdf2, 'brand_new_column', sdf2$mpg * sdf2$qsec)
head(sdf2)
```

Alternatively use `SparkR::mutate` (not sparkylr or dplyr).

Note that you don't need quotation marks, and you don't
need to mention the df when defining the new of the new/overwritten column,
but you DO need to mention it in the definition of what that column should be. Eg:
```R
# Update an existing column in place
SparkR::mutate(df, whatever = df$whatever + foo)

# Redefine variable sdf2 because we're adding a new column
sdf2 <- SparkR::mutate(sdf2, carbPlus4 = sdf2$carb + 4)
sdf2 <- withColumn(sdf2, 'new_am', ifelse(sdf2\$am == 1, 'yes', 'no'))

# Another alternative is to use a function that could be more involved
refactoring_function <- function(x){
    if(x < 2){ 
        'yes' 
    } else { 
        'no' 
    }
}

sdf2 <- withColumn(sdf2, 'new_am', refactoring_function(sdf2$am))

# Paste something to the column values
x3$pasted <- concat_ws(sep='_', lit('same_every_time'), x3$am)
```

## Frequency table
---------------

### sparklyr/dplyr

```R
iris %>%
    group_by(Species) %>%
    summarise(ct = n()) %>%
    mutate(freq = ct/sum(ct))
```

### Base R

```R
table(iris$Species)/nrow(iris)
```