R, SparkR, sparklyr/dplyr Notes
===============================

Sparkylr and dplyr syntax are more or less identical, so if you can do
something in a normal R dplyr context, you can almost surely do it in
sparklyr too.

Count occurrences of a categorical variable {#count-occurrences-of-a-categorical-variable .tabset .tabset-fade}
-------------------------------------------

### sparklyr/dplyr

`{r eval=FALSE} iris %>%   count(Species)`

### SQL

`{sql eval=FALSE} SELECT Species, count(*) FROM iris GROUP BY Species`

### Base R

`{r eval=FALSE} table iris$Species`

Count distinct {#count-distinct .tabset .tabset-fade}
--------------

### sparklyr/dplyr

`{r eval=FALSE} iris %>%   summarize(num_levels = n_distinct(Species))`
\#\#\# SQL
`{sql eval=FALSE} SELECT COUNT(DISTINCT Species) AS num_levels FROM iris`
\#\#\# Base R `{r eval=FALSE} ct <- length(unique(iris$Species))` \#\#
Distinct values {.tabset .tabset-fade}

### sparklyr/dplyr

`{r eval=FALSE} iris %>% distinct(Species)` \#\#\# SQL
`{sql eval=FALSE} SELECT DISTINCT Species FROM iris` \#\#\# Base R
`{r eval=FALSE} unique(iris$Species)` \#\#\# SparkR
`{r eval=FALSE} distinct(iris[,'Species']) # SparkR DataFrame`

### Change an R dataframe into a SparkR DataFrame

See https://spark.rstudio.com/,
https://spark.rstudio.com/deployment.html

\`\`\`{r eval=FALSE} library(sparklyr) library(dplyr)

https://spark.rstudio.com/reference/sparklyr/latest/spark\_connect.html
=======================================================================

sc \<- spark\_connect(master = "local") \# or
'url.to.remote.spark.cluster'

iris\_spark \<- copy\_to(sc, iris) \`\`\`

This method silently changes full stops to underscores in column names.
Eg `iris$Sepal.Width` becomes `iris_spark$Sepal_Width` This is because
Spark (which is built in Scala) can't handle full stops in variable
names.

Partitioning Spark DataFrames {#partitioning-spark-dataframes .tabset .tabset-fade}
-----------------------------

### SparkR

`{r eval=FALSE} getNumPartitions(df1) df1 <- repartition(df1, 100) getNumPartitions(df1)`

### sparklyr

`{r eval=FALSE} df1 <- sdf_repartition(df1, partitions=100, partition_by=column_name) sdf_num_partitions(df1)`

Note that in the sparklyr case, there is also a function called
`sdf_partition` which is used to split a DataFrame into training,
validation and testing sets. This is not what we want. We want to use
`sdf_repartition` to split the data across the different nodes in the
Spark cluster.

Add or alter a Spark DataFrame column {#add-or-alter-a-spark-dataframe-column .tabset .tabset-fade}
-------------------------------------

### sparklyr

\`\`\`{r eval=FALSE} \# Add a new column iris\_spark %\>%
mutate(sum\_everything = Sepal.Length + Sepal.Width + Petal.Length +
Petal.Width)

Update a column
===============

iris\_spark \<- iris\_spark %\>% mutate(Petal\_Width = exp(Petal\_Width)

Update a column, changing its type
==================================

iris\_spark \<- iris\_spark %\>% mutate(Sepal\_Length =
paste0('string\_now\_', Sepal\_Length))

\`\`\`

Doing this with sparklyr/dplyr/magrittr means that you don't have to
mess around with schemas and dapply like you would in SparkR.

### SparkR

\`\`\`{r eval = FALSE} \# Create a standard SparkR DataFrame sdf \<-
createDataFrame(mtcars)

Change one of the columns from DoubleType to StringType for the craic
=====================================================================

sdf2 \<- withColumn(sdf, 'cyl', cast(sdf\$cyl, 'string'))

Check:
======

printSchema(sdf) printSchema(sdf2)

Add a new column without collecting or passing a schema as an argument
======================================================================

sdf2 \<- withColumn(sdf2, 'brand\_new\_column', sdf2$mpg * sdf2$qsec)
head(sdf2)

Alternatively use SparkR::mutate (not sparkylr or dplyr)
========================================================

Note that you don't need quotation marks, and you don't
=======================================================

need to mention the df when defining the new of the new/overwritten column,
===========================================================================

but you DO need to mention it in the definition of what that column should be
=============================================================================

Eg SparkR::mutate(df, whatever = df$whatever + foo) sdf2 <- SparkR::mutate(sdf2, carbPlus4 = sdf2$carb + 4)
===========================================================================================================

Refactor something
==================

sdf2 \<- withColumn(sdf2, 'new\_am', ifelse(sdf2\$am == 1, 'yes', 'no'))

Alternatively, using a function that could be more involved
===========================================================

refactoring\_function \<- function(x){ if(x \< 2){ 'yes' } else { 'no' }
}

sdf2 \<- withColumn(sdf2, 'new\_am', refactoring\_function(sdf2\$am))

Paste something to the column values
====================================

x3$pasted <- concat_ws(sep='_', lit('same_every_time'), x3$am) \`\`\`

Use the withColumn() function, similar to PySpark but with some extra
fluff. Can mostly avoid explicitly putting together a schema this way
too.

Read from S3 bucket {#read-from-s3-bucket .tabset .tabset-fade}
-------------------

### SparkR

`{r eval=FALSE} s3url <- "s3://leotmp/*" lines <- SparkR:::textFile(sc, s3url) df1 <- SparkR:::toDF(lines) head(df1)`

Note the `*` if you want to read every file in the bucket.

### sparkylr

\`\`\`{r eval=FALSE} \# see
https://spark.rstudio.com/reference/sparklyr/latest/spark\_read\_csv.html

s3url \<- "s3://leotmp/\*\" df1 \<- spark\_read\_csv(sc,name =
"df1",path = s3url) \`\`\`

This sparklyr code is untested. Sorry.

Frequency table {#frequency-table .tabset .tabset-fade}
---------------

### sparklyr/dplyr

`{r eval=FALSE} iris %>%   group_by(Species) %>%   summarise(ct = n()) %>%   mutate(freq = ct/sum(ct))`

### Base R

`{r eval=FALSE} table(iris$Species)/nrow(iris)`

Chaining several functions together in dplyr {#chaining-several-functions-together-in-dplyr .tabset .tabset-fade}
--------------------------------------------

The use of the pipe `%>%` makes it much easier to write SQL-style or
Pandas-style code in R, instead of nesting several function calls within
one another in the traditional R way. As an example, we will count
occurrences of a variable, order by descending count, and view the first
10 rows.

### sparklyr/dplyr

`{r eval=FALSE} flight.data %>%    select(SuperPNR_ID) %>%   group_by(SuperPNR_ID) %>%    summarise(ct = n()) %>%    arrange(desc(ct)) %>%    top_n(n=10)`

### SQL

`{sql eval=FALSE} SELECT SuperPNR_ID, COUNT(SuperPNR_ID) AS ct FROM flight.data GROUP BY SuperPNR_ID ORDER BY ct DESC LIMIT 10;`
